<!DOCTYPE html>
<html>
<head>
	<title>JS Numbers</title>
</head>
<body>

	<h1>JavaScript Numbers</h1>
	<p>JavaScript has only one type of number. Numbers can be written with, or without, decimals</p>
	<p>JavaScript Numbers are Always 64-bit Floating Point.</p>
	<p>JS doesn't define different types of numbers (ie: integers, short, long, floating-point etc)</p>
	<p>This differes from other programming languages.</p>

	<h2>Precision</h2>
	<p>Integerts (numbers w/o a period or exponent notation) are considered accurate up to 15 digits.</p>
	<p>Max number of decimals is 17, but floating point arithmetic is not always 100% accurate: </p>
	<p>Example</p>
	<div id="fpex"></div>
	<p>Use the long way to solve the problem above.</p>
	<p>f = (0.2 * 10 + 0.1 * 10) / 10;</p>
	<div id="solve"></div>

	
	<script>
		// Examples of JavaScript Numbers
		var x = 34.00;
		var y = 34; 

		// Scientific Notation 
		var a = 123e5;
		var b = 123e-5; 

		// console.log(a); 

		// Precision
		var c = 999999999999999;   // Maintains accuracy
		var d = 99999999999999999; // Rounds to 100000000000000000

		// console.log(d); 

		// Floating Point Decimal Accuracy
		var e = 0.2 + 0.1; 
		// console.log(e); 
		var accurate = "Notice the following mathmatical inaccuracy. This happens b/c JS uses floating point arithmetic: 0.2 + 0.1 = " + e; 

		document.getElementById("fpex").innerHTML = accurate; 

		var f = (0.2 * 10 + 0.1 * 10) / 10; 

		var redo = "Using the formula above, notice that 0.2 + 0.1 now equals " + f; 

		document.getElementById("solve").innerHTML = redo; 



	</script>

</body>
</html>